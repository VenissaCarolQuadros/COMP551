{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4ceDp9oJjc1LLO/8gwiNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenissaCarolQuadros/COMP551/blob/dev1/COMP551_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JQoT2qUqK0k0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(url, dest):\n",
        "  response = requests.get(url)\n",
        "  with open(dest, 'wb') as file:\n",
        "      file.write(response.content)\n",
        "  return"
      ],
      "metadata": {
        "id": "0s9G1-4lRQpN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dest1= 'LinearRegression.xlsx'\n",
        "read_data(\"https://raw.githubusercontent.com/VenissaCarolQuadros/COMP551/main/data/ENB2012_data.xlsx\", dest1)\n",
        "df1= pd.read_excel(dest1)\n",
        "\n",
        "dest2= 'LogisticRegression.csv'\n",
        "read_data('https://raw.githubusercontent.com/VenissaCarolQuadros/COMP551/main/data/Qualitative_Bankruptcy/Qualitative_Bankruptcy.data.txt', dest2)\n",
        "df2= pd.read_csv(dest2, header=None)\n"
      ],
      "metadata": {
        "id": "tZ2dKvEwRkDm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.shape, df2.shape) ##Verifying dataset size\n",
        "print( df1[df1.eq('?').any(1)],  df2[ df2.eq('?').any(1)]) #Checking for nulls\n",
        "##No nulls found. Dataset size seems right"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOYCyYATSIGM",
        "outputId": "4b1f45f0-4560-4db6-856c-c797a1d4f67f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 10) (250, 7)\n",
            "Empty DataFrame\n",
            "Columns: [X1, X2, X3, X4, X5, X6, X7, X8, Y1, Y2]\n",
            "Index: [] Empty DataFrame\n",
            "Columns: [0, 1, 2, 3, 4, 5, 6]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression"
      ],
      "metadata": {
        "id": "DDUg0W2RYIKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    \n",
        "    def __init__(self, learning_rate=.0001, max_iters=1e4, epsilon=1e-8, record_history=False):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.record_history = record_history\n",
        "        self.epsilon = epsilon\n",
        "        if record_history:\n",
        "            self.w_history = []                 #to store the weight history for visualization\n",
        "            \n",
        "    def run(self, gradient_fn, x, y, w):\n",
        "        grad = np.inf\n",
        "        t = 1\n",
        "        \n",
        "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
        "            print(np.linalg.norm(grad))\n",
        "            grad = gradient_fn(x, y, w)               # compute the gradient with present weight\n",
        "            w = w - self.learning_rate * grad         # weight update step\n",
        "            if self.record_history:\n",
        "                self.w_history.append(w)\n",
        "            t += 1\n",
        "        return w\n",
        "\n",
        "class LinearRegression:\n",
        "  def __init__(self):\n",
        "    ()\n",
        "  \n",
        "  def editData(self):\n",
        "    self.X= np.c_[np.ones((self.X).shape[0]), (self.X)] \n",
        "    return\n",
        "  \n",
        "  def fitClosedForm(self, X, Y):\n",
        "    self.X=X\n",
        "    self.Y=Y\n",
        "    self.editData()\n",
        "    self.w= np.linalg.inv((self.X).T @ (self.X))@ (self.X).T@ (self.Y)\n",
        "    return\n",
        "\n",
        "  \"\"\"\n",
        "  def fitGD(self, X, Y, iterations=1000, alpha=0.01):\n",
        "    self.fitClosedForm(X, Y) #Is this a good way tp find an initial w?\n",
        "    gradJ= (1/len(self.Y))*np.sum(((self.X @ self.w)- Y)*self.X)\n",
        "    self.w= \n",
        "\n",
        "    return\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def fitGD(self, X, Y, optimizer1, optimizer2):\n",
        "        self.fitClosedForm(X, Y)\n",
        "        def gradient(x, y, w):                          # define the gradient function\n",
        "            yh =  x @ w \n",
        "            N, D = x.shape\n",
        "            grad = .5*np.dot(yh - y, x)/N\n",
        "            return grad\n",
        "        w0 = np.zeros(self.X.shape[1])                            # initialize the weights \n",
        "        self.w[:,0] = optimizer1.run(gradient, self.X, self.Y[:,0], w0)      # run the optimizer to get the optimal weights\n",
        "        self.w[:,1] = optimizer2.run(gradient, self.X, self.Y[:,1], w0) \n",
        "        return\n",
        "\n",
        "  def predict(self, Xt):\n",
        "    Xt= np.c_[np.ones((Xt).shape[0]), (Xt)] \n",
        "    return Xt @ self.w\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "tpRIjs7tyTre"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train test split\n",
        "train=df1.sample(frac=0.8,random_state=200)\n",
        "test=df1.drop(train.index)\n",
        "\n",
        "model1= LinearRegression()\n",
        "\n",
        "X_train= train.iloc[:, :8].to_numpy(copy=True) \n",
        "Y_train= train.iloc[:, 8:].to_numpy(copy=True)\n",
        "\n",
        "model1.fitClosedForm(X_train, Y_train)\n",
        "\n",
        "X_test= test.iloc[:, :8].to_numpy(copy=True)\n",
        "Y_test= test.iloc[:, 8:].to_numpy(copy=True)\n",
        "\n",
        "Ypredict=model1.predict(X_test)\n",
        "#print(np.c_[Y_test, Ypredict])\n",
        "loss=np.sum((Y_test-Ypredict)**2, axis=0)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "EYH4qAcryVXx",
        "outputId": "3d6b759a-617a-4439-fd92-b95ae83d60ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[306629.53199871 378009.33937901]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = GradientDescent(learning_rate=.00001, max_iters=100, record_history=True)\n",
        "optimizer2 = GradientDescent(learning_rate=.00001, max_iters=100, record_history=True)\n",
        "model= LinearRegression()\n",
        "model.fitGD(X_train, Y_train, optimizer1, optimizer2)"
      ],
      "metadata": {
        "id": "_uoi9IfxEKeM",
        "outputId": "36ceb425-8bdb-45bd-9397-374e8b7002f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inf\n",
            "8406.8138023492\n",
            "16279.709307489291\n",
            "31536.295765228348\n",
            "61096.03964402311\n",
            "118365.54816387239\n",
            "229319.0377322104\n",
            "444278.7716332435\n",
            "860738.4083358252\n",
            "1667580.6474034342\n",
            "3230743.8127793665\n",
            "6259190.923821794\n",
            "12126455.50313207\n",
            "23493599.24049835\n",
            "45516120.122076005\n",
            "88182196.76857124\n",
            "170842765.2913631\n",
            "330988017.10572344\n",
            "641250844.194639\n",
            "1242349039.6301694\n",
            "2406906985.3755655\n",
            "4663102760.536846\n",
            "9034220053.972492\n",
            "17502752174.863613\n",
            "33909549674.959206\n",
            "65695814445.10662\n",
            "127278010972.61467\n",
            "246586364960.6015\n",
            "477732445061.2747\n",
            "925551131347.7902\n",
            "1793147829072.6091\n",
            "3474015673478.317\n",
            "6730501916183.237\n",
            "13039565822796.209\n",
            "25262644445313.004\n",
            "48943439761972.695\n",
            "94822230543575.64\n",
            "183707059597492.56\n",
            "355911093342690.56\n",
            "689536409988450.9\n",
            "1335896715761994.8\n",
            "2588144743819365.0\n",
            "5014229869663981.0\n",
            "9714488050126318.0\n",
            "1.8820692415198576e+16\n",
            "3.646290583299522e+16\n",
            "7.064264546995141e+16\n",
            "1.3686192158819822e+17\n",
            "2.6515407876084736e+17\n",
            "5.1370523420720614e+17\n",
            "9.952442326557454e+17\n",
            "1.92817011912126e+18\n",
            "3.735605679774979e+18\n",
            "7.237302173900915e+18\n",
            "1.4021432465405723e+19\n",
            "2.716489703731213e+19\n",
            "5.262883324285345e+19\n",
            "1.0196225241346023e+20\n",
            "1.975400227714132e+20\n",
            "3.827108530154347e+20\n",
            "7.414578319923004e+20\n",
            "1.43648844105435e+21\n",
            "2.783029529458373e+21\n",
            "5.391796509098571e+21\n",
            "1.0445979565723604e+22\n",
            "2.023787227566533e+22\n",
            "3.920852722994696e+22\n",
            "7.596196806667278e+22\n",
            "1.471674913653731e+23\n",
            "2.851199233775437e+23\n",
            "5.523867394395487e+23\n",
            "1.0701851568072088e+24\n",
            "2.0733594564787831e+24\n",
            "4.016893159493164e+24\n",
            "7.782263998827296e+24\n",
            "1.5077232712628365e+25\n",
            "2.9210387402047255e+25\n",
            "5.659173327364114e+25\n",
            "1.0963991099585626e+26\n",
            "2.124145946379467e+26\n",
            "4.1152860856397885e+26\n",
            "7.972888866476693e+26\n",
            "1.5446546255679208e+27\n",
            "2.9925889501865756e+27\n",
            "5.797793549795057e+27\n",
            "1.1232551682030857e+28\n",
            "2.1761764403279625e+28\n",
            "4.216089125157939e+28\n",
            "8.168183048887426e+28\n",
            "1.582490605381415e+29\n",
            "3.065891767033233e+29\n",
            "5.939809244489401e+29\n",
            "1.1507690597656808e+30\n",
            "2.2294814099334368e+30\n",
            "4.3193613132515874e+30\n",
            "8.368260919911187e+30\n",
            "1.6212533693088152e+31\n",
            "3.140990120469477e+31\n",
            "6.085303582803304e+31\n",
            "inf\n",
            "9272.992993891581\n",
            "17959.184567488806\n",
            "34790.759717227396\n",
            "67401.52270320457\n",
            "130581.85747688686\n",
            "252986.8021857471\n",
            "490132.35847702035\n",
            "949574.4285614846\n",
            "1839690.1301250064\n",
            "3564185.9529916635\n",
            "6905196.349119558\n",
            "13378015.992773794\n",
            "25918352.341153964\n",
            "50213797.65874964\n",
            "97283401.4361637\n",
            "188475292.3760073\n",
            "365148990.59714895\n",
            "707433763.0853267\n",
            "1370570758.8967416\n",
            "2655321675.559139\n",
            "5144377373.386991\n",
            "9966633724.045261\n",
            "19309195375.742096\n",
            "37409323587.26728\n",
            "72476219957.61635\n",
            "140414259217.7914\n",
            "272036320371.1647\n",
            "527038778065.2562\n",
            "1021076425403.5299\n",
            "1978216992575.3728\n",
            "3832565684950.9556\n",
            "7425150923580.462\n",
            "14385367602291.547\n",
            "27869979099801.5\n",
            "53994847854957.875\n",
            "104608747083732.64\n",
            "202667298847172.2\n",
            "392644354961367.1\n",
            "760702838395673.2\n",
            "1473773405962171.2\n",
            "2855264818916827.0\n",
            "5531743993454446.0\n",
            "1.071711156400822e+16\n",
            "2.0763159034710396e+16\n",
            "4.022620932196775e+16\n",
            "7.793360893251645e+16\n",
            "1.509873165684929e+17\n",
            "2.925203910971788e+17\n",
            "5.6672428620075437e+17\n",
            "1.0979624885810296e+18\n",
            "2.1271748108991537e+18\n",
            "4.1211541588926653e+18\n",
            "7.984257576921589e+18\n",
            "1.5468571811872887e+19\n",
            "2.9968561459076067e+19\n",
            "5.8060607459382395e+19\n",
            "1.1248568414456087e+20\n",
            "2.1792794962267023e+20\n",
            "4.222100935591593e+20\n",
            "8.179830233427305e+20\n",
            "1.584747112122656e+21\n",
            "3.070263486787334e+21\n",
            "5.948278943807867e+21\n",
            "1.1524099656466653e+22\n",
            "2.232660474512954e+22\n",
            "4.325520381677057e+22\n",
            "8.380193399708556e+22\n",
            "1.6235651486929482e+23\n",
            "3.1454689245501516e+23\n",
            "6.093980745568377e+23\n",
            "1.1806379976451108e+24\n",
            "2.2873490082769354e+24\n",
            "4.431473064649037e+24\n",
            "8.58546441826262e+24\n",
            "1.663334024644268e+25\n",
            "3.2225165031889704e+25\n",
            "6.243251481341033e+25\n",
            "1.209557469160967e+26\n",
            "2.3433771258062944e+26\n",
            "4.540021035595269e+26\n",
            "8.795763505865735e+26\n",
            "1.7040770305808873e+27\n",
            "3.301451345544739e+27\n",
            "6.396178571389881e+27\n",
            "1.2391853168551475e+28\n",
            "2.4007776399146286e+28\n",
            "4.651227865531426e+28\n",
            "9.011213823978022e+28\n",
            "1.745818027605381e+29\n",
            "3.382319679732611e+29\n",
            "6.55285157731937e+29\n",
            "1.2695388922483986e+30\n",
            "2.459584167158283e+30\n",
            "4.7651586826314e+30\n",
            "9.231941550872798e+30\n",
            "1.7885814612928461e+31\n",
            "3.4651688662153786e+31\n",
            "6.713362254526016e+31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypredict=model.predict(X_test)\n",
        "#print(np.c_[Y_test, Ypredict])\n",
        "loss=np.sum((Y_test-Ypredict)**2, axis=0)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "d-wBmN3x1eO3",
        "outputId": "7c3c661b-1b6a-4f7f-c019-36cce6bd90af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.66480409e+292 4.46032424e+292]\n"
          ]
        }
      ]
    }
  ]
}